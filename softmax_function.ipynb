{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaomiJSang/high_perfomance_computing/blob/main/softmax_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Name: Naomi Sang**"
      ],
      "metadata": {
        "id": "ITC_H5nmouD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For project 3, you will write a parallel version of the softmax function that will work on a GPU. The suggested environment is numba/cuda.\n",
        "\n",
        "\n",
        "You can find more information about the softmax function in\n",
        " [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)"
      ],
      "metadata": {
        "id": "SE0L4pksGB7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9mhFpyvfIIh",
        "outputId": "2dcac708-c89a-4ead-835f-28a6360ed67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Kernel for exponentiation using math.exp\n",
        "@cuda.jit\n",
        "def expon_kernel(input_values, exp_results):\n",
        "    thread_idx = cuda.grid(1)  # Get the thread ID\n",
        "    if thread_idx < input_values.size:\n",
        "        exp_results[thread_idx] = math.exp(input_values[thread_idx])\n",
        "\n",
        "# Kernel for normalization\n",
        "@cuda.jit\n",
        "def normalize_kernel(exp_values, total_sum, softmax_results):\n",
        "    thread_idx = cuda.grid(1)\n",
        "    if thread_idx < exp_values.size:\n",
        "        softmax_results[thread_idx] = exp_values[thread_idx] / total_sum\n",
        "\n",
        "def softmax_gpu(input_values):\n",
        "    n = input_values.size\n",
        "\n",
        "    # Allocate device memory and transfer data\n",
        "    d_input_values = cuda.to_device(input_values)\n",
        "    d_exp_values = cuda.device_array(n, dtype=np.float64)\n",
        "    d_softmax_results = cuda.device_array(n, dtype=np.float64)\n",
        "\n",
        "    # Launch the exponentiation kernel\n",
        "    threads_per_block = 32\n",
        "    blocks_per_grid = (n + (threads_per_block - 1)) // threads_per_block\n",
        "    expon_kernel[blocks_per_grid, threads_per_block](d_input_values, d_exp_values)\n",
        "\n",
        "    # Copy exponentiated results back to host and print them\n",
        "    exp_values_host = d_exp_values.copy_to_host()\n",
        "    print(\"The array after calling expon_kernel: \", exp_values_host)\n",
        "\n",
        "    # Calculate the sum of the exponentiated values\n",
        "    total_sum = np.sum(exp_values_host)\n",
        "    print(\"The sum is: \", total_sum)\n",
        "\n",
        "    # Launch the normalization kernel\n",
        "    normalize_kernel[blocks_per_grid, threads_per_block](d_exp_values, total_sum, d_softmax_results)\n",
        "\n",
        "    # Copy the softmax results back to host and print them\n",
        "    softmax_results_host = d_softmax_results.copy_to_host()\n",
        "    return softmax_results_host\n",
        "\n",
        "# Test the softmax function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize array as in the original example\n",
        "    a = np.zeros(4, dtype=np.float64)\n",
        "    for i in range(4):\n",
        "        a[i] = (i + 1) * 1.0\n",
        "\n",
        "    print(\"The original array: \", a)\n",
        "\n",
        "    # Call the GPU softmax function\n",
        "    result = softmax_gpu(a)\n",
        "\n",
        "    # Print results as in the original example\n",
        "    print(\"The result: \", result)\n",
        "    print(\"The sum of the values in result is: \", np.sum(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLZAFE_oXPg",
        "outputId": "cc54e849-5373-4973-b98f-a2ee485f089c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original array:  [1. 2. 3. 4.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The array after calling expon_kernel:  [ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
            "The sum is:  84.7910248837216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result:  [0.0320586  0.08714432 0.23688282 0.64391426]\n",
            "The sum of the values in result is:  1.0\n"
          ]
        }
      ]
    }
  ]
}